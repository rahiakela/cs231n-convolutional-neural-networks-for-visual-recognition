{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural-networks-part-3-learning-and-evaluation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNru9JX10VhuuRbmlU8S2UU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/cs231n-convolutional-neural-networks-for-visual-recognition/blob/master/module-1-neural-networks/neural_networks_part_3_learning_and_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ey6JYRgsEZ-",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks Part 3: Learning and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhWY-0lKsFOs",
        "colab_type": "text"
      },
      "source": [
        "Table of Contents:\n",
        "\n",
        "- Gradient checks\n",
        "- Sanity checks\n",
        "- Babysitting the learning process\n",
        "  - Loss function\n",
        "  - Train/val accuracy\n",
        "  - Weights:Updates ratio\n",
        "  - Activation/Gradient distributions per layer\n",
        "  - Visualization\n",
        "- Parameter updates\n",
        "  - First-order (SGD), momentum, Nesterov momentum\n",
        "  - Annealing the learning rate\n",
        "  - Second-order methods\n",
        "  - Per-parameter adaptive learning rates (Adagrad, RMSProp)\n",
        "- Hyperparameter Optimization\n",
        "- Evaluation\n",
        "  - Model Ensembles\n",
        "- Summary\n",
        "- Additional References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmTMtYGUsxc6",
        "colab_type": "text"
      },
      "source": [
        "## Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5izYQbtQs3uz",
        "colab_type": "text"
      },
      "source": [
        "In the previous sections we’ve discussed the static parts of a Neural Networks: how we can set up the network connectivity, the data, and the loss function. This section is devoted to the dynamics, or in other words, the process of learning the parameters and finding good hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2h6Za14s6W-",
        "colab_type": "text"
      },
      "source": [
        "## Gradient Checks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR7y5I1ns_Fv",
        "colab_type": "text"
      },
      "source": [
        "In theory, performing a gradient check is as simple as comparing the analytic gradient to the numerical gradient. In practice, the process is much more involved and error prone. Here are some tips, tricks, and issues to watch out for:\n",
        "\n",
        "**Use the centered formula**. The formula you may have seen for the finite difference approximation when evaluating the numerical gradient looks as follows:\n",
        "\n",
        "$$\\frac{df(x)}{dx} = \\frac{f(x + h) - f(x)}{h}$$ **(bad, do not use)**\n",
        "\n",
        "where $h$ is a very small number, in practice approximately `1e-5` or so. In practice, it turns out that it is much better to use the centered difference formula of the form:\n",
        "\n",
        "$$\\frac{df(x)}{dx} = \\frac{f(x + h) - f(x - h)}{2h}$$ **(use instead)**\n",
        "\n",
        "This requires you to evaluate the loss function twice to check every single dimension of the gradient (so it is about 2 times as expensive), but the gradient approximation turns out to be much more precise. To see this, you can use Taylor expansion of $f(x+h)$ and $f(x−h)$ and verify that the first formula has an error on order of $O(h)$, while the second formula only has error terms on order of $O(h^2)$ (i.e. it is a second order approximation).\n",
        "\n",
        "\n",
        "**Use relative error for the comparison**. What are the details of comparing the numerical gradient $f^′_n$ and analytic gradient $f^′_a$? That is, how do we know if the two are not compatible? You might be temped to keep track of the difference $∣f^′_a−f^′_n∣$ or its square and define the gradient check as failed if that difference is above a threshold. However, this is problematic. For example, consider the case where their difference is `1e-4`. This seems like a very appropriate difference if the two gradients are about `1.0`, so we’d consider the two gradients to match. But if the gradients were both on order of `1e-5` or lower, then we’d consider `1e-4` to be a huge difference and likely a failure. Hence, it is always more appropriate to consider the relative error:\n",
        "\n",
        "$$\\frac{∣f^′_a−f^′_n∣}{max(∣f^′_a|, |f^′_n∣)}$$\n",
        "\n",
        "which considers their ratio of the differences to the ratio of the absolute values of both gradients. Notice that normally the relative error formula only includes one of the two terms (either one), but I prefer to max (or add) both to make it symmetric and to prevent dividing by zero in the case where one of the two is zero (which can often happen, especially with ReLUs). However, one must explicitly keep track of the case where both are zero and pass the gradient check in that edge case. In practice:\n",
        "\n",
        "- relative error > `1e-2` usually means the gradient is probably wrong\n",
        "- `1e-2` > relative error > `1e-4` should make you feel uncomfortable\n",
        "- `1e-4` > relative error is usually okay for objectives with kinks. But if there are no kinks (e.g. use of tanh nonlinearities and softmax), then `1e-4` is too high.\n",
        "- `1e-7` and less you should be happy.\n",
        "\n",
        "Also keep in mind that the deeper the network, the higher the relative errors will be. So if you are gradient checking the input data for a 10-layer network, a relative error of `1e-2` might be okay because the errors build up on the way. Conversely, an error of `1e-2` for a single differentiable function likely indicates incorrect gradient.\n",
        "\n",
        "**Use double precision**. A common pitfall is using single precision floating point to compute gradient check. It is often that case that you might get high relative errors (as high as `1e-2`) even with a correct gradient implementation. In my experience I’ve sometimes seen my relative errors plummet from `1e-2` to `1e-8` by switching to double precision.\n",
        "\n",
        "**Stick around active range of floating point**. It’s a good idea to read through “[What Every Computer Scientist Should Know About Floating-Point Arithmetic](https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html)”, as it may demystify your errors and enable you to write more careful code. For example, in neural nets it can be common to normalize the loss function over the batch. However, if your gradients per datapoint are very small, then additionally dividing them by the number of data points is starting to give very small numbers, which in turn will lead to more numerical issues. This is why I like to always print the raw numerical/analytic gradient, and make sure that the numbers you are comparing are not extremely small (e.g. roughly `1e-10` and smaller in absolute value is worrying). If they are you may want to temporarily scale your loss function up by a constant to bring them to a “nicer” range where floats are more dense - ideally on the order of `1.0`, where your float exponent is `0`.\n",
        "\n",
        "**Kinks in the objective**. One source of inaccuracy to be aware of during gradient checking is the problem of kinks. Kinks refer to non-differentiable parts of an objective function, introduced by functions such as $ReLU(max(0,x))$, or the SVM loss, Maxout neurons, etc. Consider gradient checking the ReLU function at $x=−1e6$. Since $x<0$, the analytic gradient at this point is exactly zero. However, the numerical gradient would suddenly compute a non-zero gradient because $f(x+h)$ might cross over the kink (e.g. if $h>1e−6)$ and introduce a non-zero contribution. You might think that this is a pathological case, but in fact this case can be very common. For example, an SVM for CIFAR-10 contains up to 450,000 $max(0,x)$ terms because there are 50,000 examples and each example yields 9 terms to the objective. Moreover, a Neural Network with an SVM classifier will contain many more kinks due to ReLUs.\n",
        "\n",
        "Note that it is possible to know if a kink was crossed in the evaluation of the loss. This can be done by keeping track of the identities of all “winners” in a function of form $max(x,y)$; That is, was x or y higher during the forward pass. If the identity of at least one winner changes when evaluating $f(x+h)$ and then $f(x−h)$, then a kink was crossed and the numerical gradient will not be exact.\n",
        "\n",
        "**Use only few datapoints**. One fix to the above problem of kinks is to use fewer datapoints, since loss functions that contain kinks (e.g. due to use of ReLUs or margin losses etc.) will have fewer kinks with fewer datapoints, so it is less likely for you to cross one when you perform the finite different approximation. Moreover, if your gradcheck for only `~2` or `3` datapoints then you would almost certainly gradcheck for an entire batch. Using very few datapoints also makes your gradient check faster and more efficient.\n",
        "\n",
        "**Be careful with the step size h**. It is not necessarily the case that smaller is better, because when $h$ is much smaller, you may start running into numerical precision problems. Sometimes when the gradient doesn’t check, it is possible that you change h to be `1e-4` or `1e-6` and suddenly the gradient will be correct. [This wikipedia article](https://en.wikipedia.org/wiki/Numerical_differentiation) contains a chart that plots the value of $h$ on the x-axis and the numerical gradient error on the y-axis.\n",
        "\n",
        "**Gradcheck during a “characteristic” mode of operation**. It is important to realize that a gradient check is performed at a particular (and usually random), single point in the space of parameters. Even if the gradient check succeeds at that point, it is not immediately certain that the gradient is correctly implemented globally. Additionally, a random initialization might not be the most “characteristic” point in the space of parameters and may in fact introduce pathological situations where the gradient seems to be correctly implemented but isn’t. For instance, an SVM with very small weight initialization will assign almost exactly zero scores to all datapoints and the gradients will exhibit a particular pattern across all datapoints. An incorrect implementation of the gradient could still produce this pattern and not generalize to a more characteristic mode of operation where some scores are larger than others. Therefore, to be safe it is best to use a short burn-in time during which the network is allowed to learn and perform the gradient check after the loss starts to go down. The danger of performing it at the first iteration is that this could introduce pathological edge cases and mask an incorrect implementation of the gradient.\n",
        "\n",
        "**Don’t let the regularization overwhelm the data**. It is often the case that a loss function is a sum of the data loss and the regularization loss (e.g. L2 penalty on weights). One danger to be aware of is that the regularization loss may overwhelm the data loss, in which case the gradients will be primarily coming from the regularization term (which usually has a much simpler gradient expression). This can mask an incorrect implementation of the data loss gradient. Therefore, it is recommended to turn off regularization and check the data loss alone first, and then the regularization term second and independently. One way to perform the latter is to hack the code to remove the data loss contribution. Another way is to increase the regularization strength so as to ensure that its effect is non-negligible in the gradient check, and that an incorrect implementation would be spotted.\n",
        "\n",
        "**Remember to turn off dropout/augmentations**. When performing gradient check, remember to turn off any non-deterministic effects in the network, such as dropout, random data augmentations, etc. Otherwise these can clearly introduce huge errors when estimating the numerical gradient. The downside of turning off these effects is that you wouldn’t be gradient checking them (e.g. it might be that dropout isn’t backpropagated correctly). Therefore, a better solution might be to force a particular random seed before evaluating both $f(x+h)$ and f$(x−h)$, and when evaluating the analytic gradient.\n",
        "\n",
        "**Check only few dimensions**. In practice the gradients can have sizes of million parameters. In these cases it is only practical to check some of the dimensions of the gradient and assume that the others are correct. **Be careful**: One issue to be careful with is to make sure to gradient check a few dimensions for every separate parameter. In some applications, people combine the parameters into a single large parameter vector for convenience. In these cases, for example, the biases could only take up a tiny number of parameters from the whole vector, so it is important to not sample at random but to take this into account and check that all parameters receive the correct gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvnZjFSW6iAu",
        "colab_type": "text"
      },
      "source": [
        "## Before learning: sanity checks Tips/Tricks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8pciEI-6i6K",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}